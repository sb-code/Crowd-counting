{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\python39\\lib\\site-packages (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python39\\lib\\site-packages (from tensorflow) (4.0.0)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Requirement already satisfied: setuptools in c:\\python39\\lib\\site-packages (from tensorflow) (56.0.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\python39\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python39\\lib\\site-packages\\numpy-1.21.3-py3.9-win-amd64.egg (from tensorflow) (1.21.3)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.0-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python39\\lib\\site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python39\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\python39\\lib\\site-packages (from tensorflow) (3.20.0)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python39\\lib\\site-packages\\six-1.16.0-py3.9.egg (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python39\\lib\\site-packages\\requests-2.26.0-py3.9.egg (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages\\certifi-2021.10.8-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python39\\lib\\site-packages\\charset_normalizer-2.0.7-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages\\idna-3.3-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages\\urllib3-1.26.7-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Installing collected packages: absl-py, wrapt, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\python39\\\\Scripts\\\\tensorboard.exe' -> 'c:\\\\python39\\\\Scripts\\\\tensorboard.exe.deleteme'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from matplotlib import cm as CM\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import scipy.io as io\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: setuptools in c:\\python39\\lib\\site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python39\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.0-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\python39\\lib\\site-packages (from tensorflow) (3.20.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python39\\lib\\site-packages (from tensorflow) (4.0.0)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.44.0-cp39-cp39-win_amd64.whl (3.4 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python39\\lib\\site-packages\\six-1.16.0-py3.9.egg (from tensorflow) (1.16.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python39\\lib\\site-packages\\numpy-1.21.3-py3.9-win-amd64.egg (from tensorflow) (1.21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python39\\lib\\site-packages\\requests-2.26.0-py3.9.egg (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages\\certifi-2021.10.8-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python39\\lib\\site-packages\\charset_normalizer-2.0.7-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages\\idna-3.3-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages\\urllib3-1.26.7-py3.9.egg (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Installing collected packages: grpcio, google-auth-oauthlib, absl-py, wrapt, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\python39\\\\Scripts\\\\google-oauthlib-tool.exe' -> 'c:\\\\python39\\\\Scripts\\\\google-oauthlib-tool.exe.deleteme'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow\n",
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\CROWD-COUNTING-USING-CSRNET-master\\ShanghaiTech\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "root = os.path.join(os.getcwd(),'ShanghaiTech')\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_A_train = os.path.join(root,'part_A/train_data','images')\n",
    "part_A_test = os.path.join(root,'part_A/test_data','images')\n",
    "part_B_train = os.path.join(root,'part_B/train_data','images')\n",
    "part_B_test = os.path.join(root,'part_B/test_data','images')\n",
    "temp = 'test_images'\n",
    "path_sets = [part_B_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images :  400\n"
     ]
    }
   ],
   "source": [
    "img_paths = []\n",
    "\n",
    "for path in path_sets:\n",
    "    \n",
    "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "        \n",
    "        img_paths.append(str(img_path))\n",
    "        \n",
    "print(\"Total images : \",len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img(path):\n",
    "    #Function to load,normalize and return image \n",
    "    im = Image.open(path).convert('RGB')\n",
    "    \n",
    "    im = np.array(im)\n",
    "    \n",
    "    im = im/255.0\n",
    "    \n",
    "    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
    "    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
    "    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
    "\n",
    "    print(im.shape)\n",
    "    #im = np.expand_dims(im,axis  = 0)\n",
    "    return im\n",
    "\n",
    "def get_input(path):\n",
    "    print(\"inp1\", path)\n",
    "    path = path[0] \n",
    "    print(\"inp2\", path)\n",
    "    img = create_img(path)\n",
    "    return(img)\n",
    "    \n",
    "    \n",
    "    \n",
    "# def get_output(path):\n",
    "#     #import target\n",
    "#     #resize target\n",
    "#     print(\"op\", path)\n",
    "#     #create a file here : - \n",
    "    \n",
    "#     gt_file = h5py.File(path,'r')\n",
    "#     print(\"op2\", path)\n",
    "    \n",
    "#     target = np.asarray(gt_file['density'])\n",
    "    \n",
    "#     img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
    "    \n",
    "#     img = np.expand_dims(img,axis  = 3)\n",
    "    \n",
    "#     #print(img.shape)\n",
    "    \n",
    "#     return img\n",
    "\n",
    "def get_output(path):\n",
    "    #import target\n",
    "    #resize target\n",
    "    \n",
    "    gt_file = h5py.File(path,'r')\n",
    "    \n",
    "    target = np.asarray(gt_file['density'])\n",
    "    print(\"op_size\", target.shape)\n",
    "    \n",
    "    img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
    "    \n",
    "    img = np.expand_dims(img,axis  = 2)\n",
    "    print(\"op_size2\",img.shape)\n",
    "    \n",
    "    #print(img.shape)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "    \n",
    "    \n",
    "def preprocess_input(image,target):\n",
    "    #crop image\n",
    "    #crop target\n",
    "    #resize target\n",
    "    crop_size = (int(image.shape[0]/2),int(image.shape[1]/2))\n",
    "    \n",
    "    \n",
    "    if random.randint(0,9)<= -1:            \n",
    "            dx = int(random.randint(0,1)*image.shape[0]*1./2)\n",
    "            dy = int(random.randint(0,1)*image.shape[1]*1./2)\n",
    "    else:\n",
    "            dx = int(random.random()*image.shape[0]*1./2)\n",
    "            dy = int(random.random()*image.shape[1]*1./2)\n",
    "\n",
    "    #print(crop_size , dx , dy)\n",
    "    img = image[dx : crop_size[0]+dx , dy:crop_size[1]+dy]\n",
    "    \n",
    "    target_aug = target[dx:crop_size[0]+dx,dy:crop_size[1]+dy]\n",
    "    #print(img.shape)\n",
    "\n",
    "    return(img,target_aug)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image data generator \n",
    "def image_generator(files, batch_size = 64):\n",
    "    print(\"outside here\")\n",
    "    while True:\n",
    "        print(\"inside here\")\n",
    "        print(\"rnewinp\", batch_size)\n",
    "        input_path = np.random.choice(a = files, size = batch_size)\n",
    "        print(input_path)\n",
    "        \n",
    "        batch_input = []\n",
    "        batch_output = [] \n",
    "          \n",
    "        #for input_path in batch_paths:\n",
    "        \n",
    "        inputt = get_input(input_path )\n",
    "        output = get_output(input_path[0].replace('.jpg','.h5').replace('images','ground-truth') )\n",
    "            \n",
    "       \n",
    "        batch_input += [inputt]\n",
    "        batch_output += [output]\n",
    "    \n",
    "\n",
    "        batch_x = np.array( batch_input )\n",
    "        batch_y = np.array( batch_output )\n",
    "        \n",
    "        yield( batch_x, batch_y )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mod(model , str1 , str2):\n",
    "    model.save_weights(str1)\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(str2, \"w\") as json_file:\n",
    "        json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vgg(model):\n",
    "    vgg =  VGG16(weights='imagenet', include_top=False)\n",
    "    \n",
    "    model_json_temp = vgg.to_json()\n",
    "    with open(\"model_json_temp\", \"w\") as json_file:\n",
    "        json_file.write(model_json_temp)\n",
    "    \n",
    "    vgg.save_weights(\"model.h5\")\n",
    "    print(\"saved to disk\")\n",
    "\n",
    "\n",
    "\n",
    "    json_file = open('model_json_temp', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights('model.h5')\n",
    "\n",
    "    print('model loaded from disk')\n",
    "    \n",
    "    vgg = loaded_model\n",
    "    \n",
    "    vgg_weights=[]                         \n",
    "    for layer in vgg.layers:\n",
    "        if('conv' in layer.name):\n",
    "            vgg_weights.append(layer.get_weights())\n",
    "    \n",
    "    \n",
    "    offset=0\n",
    "    i=0\n",
    "    while(i<10):\n",
    "        if('conv' in model.layers[i+offset].name):\n",
    "            model.layers[i+offset].set_weights(vgg_weights[i])\n",
    "            i=i+1\n",
    "            #print('h')\n",
    "            \n",
    "        else:\n",
    "            offset=offset+1\n",
    "\n",
    "    return (model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    # Euclidean distance as a measure of loss (Loss function) \n",
    "    return K.sqrt(K.sum(x=K.square(y_pred - y_true), axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model : VGG + Conv\n",
    "def CrowdNet():  \n",
    "            #Variable Input Size\n",
    "            rows = None\n",
    "            cols = None\n",
    "            \n",
    "            #Batch Normalisation option\n",
    "            \n",
    "            batch_norm = 0\n",
    "            kernel = (3, 3)\n",
    "            init = RandomNormal(stddev=0.01)\n",
    "            model = Sequential() \n",
    "            \n",
    "            #custom VGG:\n",
    "            \n",
    "            if(batch_norm):\n",
    "                model.add(Conv2D(64, kernel_size = kernel, input_shape = (rows,cols,3),activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))            \n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                \n",
    "            else:\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same',input_shape = (rows, cols, 3), kernel_initializer = init))\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))            \n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "            #Conv2D\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(256, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(128, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(64, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(1, (1, 1), activation='relu', dilation_rate = 1, kernel_initializer = init, padding = 'same'))\n",
    "        \n",
    "            sgd = SGD(lr = 1e-19, decay = (5*1e-4), momentum = 0.95)\n",
    "            model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])\n",
    "            \n",
    "            model = init_weights_vgg(model)\n",
    "            \n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to disk\n",
      "model loaded from disk\n"
     ]
    }
   ],
   "source": [
    "model = CrowdNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_17 (Conv2D)          (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, None, None, 64)   0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, None, None, 128)  0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, None, None, 256)  0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, None, None, 256)   1179904   \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, None, None, 128)   295040    \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, None, None, 64)    73792     \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, None, None, 1)     65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,263,489\n",
      "Trainable params: 16,263,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = image_generator(img_paths,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen2 = train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(list(train_gen)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr = 1e-19, decay = (5*1e-4), momentum = 0.95)\n",
    "model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_10952\\888474770.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_gen,steps_per_epoch= 400 , verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outside here\n",
      "inside here\n",
      "rnewinp 1\n",
      "['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_316.jpg']\n",
      "inp1 ['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_316.jpg']\n",
      "inp2 c:\\Users\\hp\\Desktop\\CROWD-COUNTING-USING-CSRNET-master\\ShanghaiTech\\part_B/train_data\\images\\IMG_316.jpg\n",
      "(768, 1024, 3)\n",
      "op_size (768, 1024)\n",
      "op_size2 (96, 128, 1)\n",
      "inside here\n",
      "rnewinp 1\n",
      "['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_133.jpg']\n",
      "inp1 ['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_133.jpg']\n",
      "inp2 c:\\Users\\hp\\Desktop\\CROWD-COUNTING-USING-CSRNET-master\\ShanghaiTech\\part_B/train_data\\images\\IMG_133.jpg\n",
      "(768, 1024, 3)\n",
      "op_size (768, 1024)\n",
      "op_size2 (96, 128, 1)\n",
      "  1/400 [..............................] - ETA: 2:12:36 - loss: 0.0027 - mse: 3.9275e-05inside here\n",
      "rnewinp 1\n",
      "['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_237.jpg']\n",
      "inp1 ['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_237.jpg']\n",
      "inp2 c:\\Users\\hp\\Desktop\\CROWD-COUNTING-USING-CSRNET-master\\ShanghaiTech\\part_B/train_data\\images\\IMG_237.jpg\n",
      "(768, 1024, 3)\n",
      "op_size (768, 1024)\n",
      "op_size2 (96, 128, 1)\n",
      "  2/400 [..............................] - ETA: 1:18:39 - loss: nan - mse: nan          inside here\n",
      "rnewinp 1\n",
      "['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_293.jpg']\n",
      "inp1 ['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_293.jpg']\n",
      "inp2 c:\\Users\\hp\\Desktop\\CROWD-COUNTING-USING-CSRNET-master\\ShanghaiTech\\part_B/train_data\\images\\IMG_293.jpg\n",
      "(768, 1024, 3)\n",
      "op_size (768, 1024)\n",
      "op_size2 (96, 128, 1)\n",
      "  3/400 [..............................] - ETA: 1:20:38 - loss: nan - mse: naninside here\n",
      "rnewinp 1\n",
      "['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_361.jpg']\n",
      "inp1 ['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_361.jpg']\n",
      "inp2 c:\\Users\\hp\\Desktop\\CROWD-COUNTING-USING-CSRNET-master\\ShanghaiTech\\part_B/train_data\\images\\IMG_361.jpg\n",
      "(768, 1024, 3)\n",
      "op_size (768, 1024)\n",
      "op_size2 (96, 128, 1)\n",
      "  4/400 [..............................] - ETA: 1:21:45 - loss: nan - mse: naninside here\n",
      "rnewinp 1\n",
      "['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_167.jpg']\n",
      "inp1 ['c:\\\\Users\\\\hp\\\\Desktop\\\\CROWD-COUNTING-USING-CSRNET-master\\\\ShanghaiTech\\\\part_B/train_data\\\\images\\\\IMG_167.jpg']\n",
      "inp2 c:\\Users\\hp\\Desktop\\CROWD-COUNTING-USING-CSRNET-master\\ShanghaiTech\\part_B/train_data\\images\\IMG_167.jpg\n",
      "(768, 1024, 3)\n",
      "op_size (768, 1024)\n",
      "op_size2 (96, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen,steps_per_epoch= 400 , verbose=1)\n",
    "#,steps_per_epoch= 700 , verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mod(model,\"weights/model_A_weights.h5\",\"models/Model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
